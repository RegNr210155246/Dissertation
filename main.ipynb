{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Biased Hopfield networks","metadata":{}},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n#%matplotlib widget\nimport sys\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as pl\nimport pandas as pd\nfrom scipy.stats import kurtosis, skew\nfrom matplotlib.ticker import FormatStrFormatter\nimport math\nfrom numpy import trapz\nfrom tqdm import tqdm\nfrom scipy.stats import norm\nimport matplotlib.ticker as mtick\nfrom matplotlib.ticker import PercentFormatter\n\n\ndef learn_patterns(patterns, sparseness):\n    N = patterns.size(dim=1)\n    return torch.matmul(patterns-sparseness, patterns.permute(0,2,1)-sparseness)/N\n\ndef activation_function(activity,threshold,stochastic,temperature):\n    output = activity.clone()\n    if(stochastic=='on'):\n        temp = torch.rand(output.size()).cuda()\n        threshold_stochastic = 0.125*torch.log(temp/(1-temp))*temperature/2\n    else:\n        threshold_stochastic = torch.zeros(output.size()).cuda()\n    output[output-threshold-threshold_stochastic<= 0] = -1\n    output[output-threshold-threshold_stochastic > 0] = 1\n    return(output)\n  \ndef network_output(state, weights, threshold, sparseness_ratio, stochastic, temperature):\n    return(activation_function(torch.matmul(weights,state/2+1/2-sparseness_ratio) ,threshold, stochastic, temperature))\n\ndef patterns_distance(pattern1, pattern2):\n    return((1-torch.sum(pattern1*pattern2,1)/pattern1.size(dim=1))/2)\n    \n#####\ndef set_up_the_model(iterations, N, nr_patterns, sparseness_ratio):\n    W = torch.zeros((iterations,N,N)).cuda()\n    patterns = torch.rand(iterations,N,nr_patterns).cuda()\n    patterns[patterns<sparseness_ratio] = 1\n    patterns[patterns!=1] = 0\n    W = learn_patterns(patterns, sparseness_ratio)\n    W = W*(1-torch.eye(N,N).cuda())\n    patterns = patterns*2-1\n    return patterns, W\n          \ndef update_network(starting_state, W, threshold, update_steps, sparseness_ratio, stochastic='off', temperature=0):\n    # update the network based on the W dynamics\n    update_state = starting_state.clone()\n    for i in range(update_steps):\n        update_state = network_output(update_state, W, threshold, sparseness_ratio, stochastic, temperature)\n    return update_state.squeeze()\n\n#####\ndef disturb_a_pattern(pattern, nr_flips, how='flip'):\n    if how=='on':\n        return turn_nodes_on(pattern, nr_flips)\n    elif how=='off':\n        return turn_nodes_off(pattern, nr_flips)\n    else:\n        return flip_nodes(pattern, nr_flips)     #default is flip\n    \ndef flip_nodes(state, nr_flips): \n    output = state.clone()\n    N = output.size(dim=1)\n    if(nr_flips>0):\n        output[:,0:nr_flips] *= -1\n    return(output)\n\ndef turn_nodes_on(state, nr_flips): \n    output = state.clone()\n    N = output.size(dim=1)\n    if(nr_flips>0):\n        output[:,0:nr_flips] = 1\n    return(output)\n\ndef turn_nodes_off(state, nr_flips): \n    output = state.clone()\n    N = output.size(dim=1)\n    if(nr_flips>0):\n        output[:,0:nr_flips] = -1\n    return(output)\n\n#####\ndef dilute_connections(W, dilution_prob, random_matrix):\n    output = W.clone()\n    output[random_matrix<dilution_prob] = 0\n    return output\n\ndef dilute_nodes_p(patterns, dilution_prob):\n    patterns_n = patterns.clone()\n    N = patterns.size(dim=1)\n    keep_index = torch.arange(int(N*dilution_prob),N).cuda()\n    patterns_n = torch.index_select(patterns_n, 1, keep_index).cuda()\n    return patterns_n\n    \ndef dilute_nodes_W(W, dilution_prob):\n    W_n = W.clone()  \n    N = patterns.size(dim=1)\n    keep_index = torch.arange(int(N*dilution_prob),N).cuda()\n    W_n = torch.index_select(W_n, 1, keep_index).cuda()\n    W_n = torch.index_select(W_n, 2, keep_index).cuda()\n    return W_n\n\n#####\ndef calc_auc(y_matrix, x, y_max):\n    output = torch.zeros(len(y_matrix))\n    for i in range(len(output)):\n        output[i] = trapz(y_matrix[i,:].numpy(), x.numpy()) / (x[-1]*y_max)\n    return output\n\ndef calc_dropping_point(y_matrix, x, threshold):\n    output = torch.zeros(len(y_matrix))\n    for i in range(len(output)):\n        output[i] = x[max(np.min(np.where(y_matrix[i,:].numpy()<threshold))-1,0)] / x[-1]\n    return output\n\ndef log2_noninf(x):\n    if x>0:\n        return(np.log2(x))\n    if x==0:\n        return(0)    \n\ndef patterns_mutual_info(pattern1, pattern2):\n    p0 = np.sum(pattern1==-1)/len(pattern1)\n    p1 = np.sum(pattern1==1)/len(pattern1)\n    H_Y = -p0 * log2_noninf(p0) -p1 * log2_noninf(p1)\n    \n    p0 = np.sum(pattern2==-1)/len(pattern2)\n    p1 = np.sum(pattern2==1)/len(pattern2)\n    H_Y_given_X = 0\n    if p0>0:\n        p00 = np.sum((pattern2==-1) & (pattern1==-1)) /np.sum(pattern2==-1)\n        p01 = np.sum((pattern2==-1) & (pattern1==1))  /np.sum(pattern2==-1)\n        H_Y_given_X = -p0 * (p00*log2_noninf(p00)+p01*log2_noninf(p01))\n    if p1>0:\n        p10 = np.sum((pattern2==1)  & (pattern1==-1)) /np.sum(pattern2==1)\n        p11 = np.sum((pattern2==1)  & (pattern1==1))  /np.sum(pattern2==1)\n        H_Y_given_X = H_Y_given_X -p1 * (p10*log2_noninf(p10)+p11*log2_noninf(p11))    \n    return(H_Y - H_Y_given_X)\n\ndef patterns_mutual_info_normalized(pattern1, pattern2):\n    normalization_factor = patterns_mutual_info(pattern1, pattern1)\n    return (patterns_mutual_info(pattern1, pattern2) / normalization_factor)\n        \ndef calc_f_score(pattern1, pattern2):\n    TP = np.sum((pattern1==1) & (pattern2==1))\n    FP = np.sum((pattern1==-1) & (pattern2==1))\n    FN = np.sum((pattern1==1) & (pattern2==-1))\n    if TP==0:\n        return 0\n    else:        \n        precision = TP / (TP + FP)\n        recall = TP / (TP + FN)\n        return(2*(precision*recall)/(precision+recall))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-11T14:44:19.686831Z","iopub.execute_input":"2022-08-11T14:44:19.687586Z","iopub.status.idle":"2022-08-11T14:44:19.753197Z","shell.execute_reply.started":"2022-08-11T14:44:19.687537Z","shell.execute_reply":"2022-08-11T14:44:19.751918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 0. determining the tolerance","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(seed=7)\nN = 5000\niterations = 100\nsparseness_ratio = np.array([0.01,0.02,0.05,0.1,0.15,0.2,0.3,0.4,0.5])\nthreshold = (sparseness_ratio**3-sparseness_ratio**2 + sparseness_ratio-2*sparseness_ratio**2+sparseness_ratio**3)/2","metadata":{"execution":{"iopub.status.busy":"2022-07-29T11:51:17.857197Z","iopub.execute_input":"2022-07-29T11:51:17.857662Z","iopub.status.idle":"2022-07-29T11:51:17.866140Z","shell.execute_reply.started":"2022-07-29T11:51:17.857627Z","shell.execute_reply":"2022-07-29T11:51:17.864549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mutual_info_temp = np.zeros(int(N/5))\nf_score_temp = np.zeros(int(N/5))\nmutual_info_quantile = np.zeros((len(sparseness_ratio),iterations))\nf_score_quantile = np.zeros((len(sparseness_ratio),iterations))\nfor s in enumerate(sparseness_ratio):\n    print(s[0])\n    for i in range(iterations):\n        pattern = np.random.rand(N)  \n        pattern[pattern<np.quantile(pattern, s[1])] = 1 #this means setting the exact number of ON nodes\n        pattern[pattern!=1] = 0\n        pattern = pattern*2-1        \n        for n in range(int(N/5)):\n            disturbed_pattern = pattern.copy()\n            disturbed_pattern[0:n] *= -1\n            mutual_info_temp[n] = patterns_mutual_info_normalized(pattern, disturbed_pattern)\n            f_score_temp[n] = calc_f_score(pattern, disturbed_pattern)\n        mutual_info_quantile[s[0],i] = np.max(np.where(mutual_info_temp>0.88))\n        f_score_quantile[s[0],i] = np.max(np.where(f_score_temp>0.88))","metadata":{"execution":{"iopub.status.busy":"2022-07-28T11:59:21.836680Z","iopub.execute_input":"2022-07-28T11:59:21.837412Z","iopub.status.idle":"2022-07-28T12:07:09.396681Z","shell.execute_reply.started":"2022-07-28T11:59:21.837373Z","shell.execute_reply":"2022-07-28T12:07:09.395700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.rcParams.update({'font.size': 11})\n#print(np.round(np.mean(mutual_info_quantile,1)/N,4))\ntolerance = np.array([0.0025,0.0041,0.007,0.0101,0.0119,0.0134,0.015,0.0159,0.0162])\n\nfig, axs = pl.subplots(1, 1, figsize=(7, 4), facecolor='w', edgecolor='k')\naxs.plot(sparseness_ratio, tolerance, '-s', color='darkgreen')\naxs.set_title(\"tolerance level based on mutual information\",size=15)\naxs.set_ylabel(\"tolerance\",size=14)\naxs.set_xlabel(\"mean activity level (p)\",size=14)\naxs.xaxis.set_major_formatter(mtick.PercentFormatter(1.0)) \naxs.set_ylim([0, 0.02])\naxs.grid(linewidth = 0.6)\naxs.yaxis.set_major_formatter(mtick.PercentFormatter(1.0)) \n\nfor i in range(len(sparseness_ratio)):\n    if i>6:        \n        pl.annotate(tolerance[i],\n                    (sparseness_ratio[i],tolerance[i]),\n                    textcoords=\"offset points\",\n                    xytext=(-21,-16))\n    elif i>5:        \n        pl.annotate(tolerance[i],\n                    (sparseness_ratio[i],tolerance[i]),\n                    textcoords=\"offset points\",\n                    xytext=(-15,-16))        \n    elif i>1:\n        pl.annotate(tolerance[i],\n            (sparseness_ratio[i],tolerance[i]),\n            textcoords=\"offset points\",\n            xytext=(3.5,-12))\n    else:\n        pl.annotate(tolerance[i],\n            (sparseness_ratio[i],tolerance[i]),\n            textcoords=\"offset points\",\n            xytext=(6,-5))    \npl.show()    ","metadata":{"execution":{"iopub.status.busy":"2022-07-29T16:30:31.286032Z","iopub.execute_input":"2022-07-29T16:30:31.286388Z","iopub.status.idle":"2022-07-29T16:30:31.512640Z","shell.execute_reply.started":"2022-07-29T16:30:31.286357Z","shell.execute_reply":"2022-07-29T16:30:31.511729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. dependence of error on threshold for different sparsness","metadata":{}},{"cell_type":"code","source":"# GENERAL SETTINGS (see methods)\nN = 5000\nupdate_steps = 50\niterations = 10\nbatches = 10\nsparseness_ratio = np.array([0.01,0.02,0.05,0.1,0.15,0.2,0.3,0.4,0.5])\nthreshold = (sparseness_ratio**3-sparseness_ratio**2 + sparseness_ratio-2*sparseness_ratio**2+sparseness_ratio**3)/2\ntolerance = np.array([0.0025,0.0041,0.007,0.0101,0.0119,0.0134,0.015,0.0159,0.0162])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(seed=7)\n\nnr_patterns = int(0.1*N)\nthresholds = np.sort(np.append(np.append([-0.2, -0.05, 0.005, 0.11, 0.2], np.linspace(-0.04, 0.1, 20)), threshold))","metadata":{"execution":{"iopub.status.busy":"2022-08-06T14:44:13.929350Z","iopub.execute_input":"2022-08-06T14:44:13.929909Z","iopub.status.idle":"2022-08-06T14:44:13.944986Z","shell.execute_reply.started":"2022-08-06T14:44:13.929877Z","shell.execute_reply":"2022-08-06T14:44:13.943494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distances1 = torch.zeros((batches,len(sparseness_ratio),iterations,len(thresholds)))\nfor b in range(batches):    \n    print(b)\n    for s in enumerate(sparseness_ratio):\n        patterns, W = set_up_the_model(iterations, N, nr_patterns, s[1])\n        for th in enumerate(thresholds):\n            distances1[b,s[0],:,th[0]] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W, th[1], update_steps, s[1])) #only the first pattern","metadata":{"execution":{"iopub.status.busy":"2022-07-29T10:45:26.570874Z","iopub.execute_input":"2022-07-29T10:45:26.571218Z","iopub.status.idle":"2022-07-29T10:53:08.638739Z","shell.execute_reply.started":"2022-07-29T10:45:26.571189Z","shell.execute_reply":"2022-07-29T10:53:08.637708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hit_ratio = torch.zeros((len(sparseness_ratio),len(thresholds)))\naverage_distance = torch.zeros((len(sparseness_ratio),len(thresholds)))\nfor s in enumerate(sparseness_ratio):\n    for th in enumerate(thresholds):\n        hit_ratio[s[0],th[0]] = torch.sum(distances1[:,s[0],:,th[0]]<=tolerance[s[0]])/(iterations*batches)\n        average_distance[s[0],th[0]] = torch.mean(distances1[:,s[0],:,th[0]])\n#####        \nfig, axs = pl.subplots(3, 3, figsize=(15, 8), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.45, wspace=0.2)\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        axs[i,j].plot(thresholds, hit_ratio[i*axs.shape[1]+j,:], color='darkgreen')\n        axs[i,j].set_title('p='+\"{:.0%}\".format(np.round(sparseness_ratio[i*axs.shape[1]+j],3)),size=14)\n        axs[i,j].set_ylim((-0.1, 1.1))\n        axs[i,j].axvline(x=threshold[i*axs.shape[1]+j], color='r', linestyle='--', linewidth=1.2)\n        axs[i,j].grid(linewidth = 0.6)\n        if j==0: axs[i,j].set_ylabel('hit ratio', fontsize=14)\n        if i==2: axs[i,j].set_xlabel('threshold', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T11:07:04.249401Z","iopub.execute_input":"2022-07-29T11:07:04.249871Z","iopub.status.idle":"2022-07-29T11:07:05.323294Z","shell.execute_reply.started":"2022-07-29T11:07:04.249830Z","shell.execute_reply":"2022-07-29T11:07:05.322383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. examining the load parameter for sparse networks","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(seed=7)\n\nnr_patterns = (np.append(np.linspace(0.05, 0.31, 14), np.append(np.linspace(0.34, 1, 12), np.linspace(1.1, 2.1, 11)))*N).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T13:13:01.750390Z","iopub.execute_input":"2022-07-28T13:13:01.751333Z","iopub.status.idle":"2022-07-28T13:13:01.758477Z","shell.execute_reply.started":"2022-07-28T13:13:01.751286Z","shell.execute_reply":"2022-07-28T13:13:01.757253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distances2 = torch.ones((batches,len(sparseness_ratio),iterations,len(nr_patterns)))\nfor b in range(batches):\n    print(b)\n    for s in enumerate(sparseness_ratio):\n        for n in enumerate(nr_patterns):\n            if(n[1]<N or s[1]<=0.02): #this is merely to make the program faster; for sparsity>2%, the performance drops to 0 earlier than load=1.\n                patterns, W = set_up_the_model(iterations, N, int(n[1]), s[1])\n                distances2[b,s[0],:,n[0]] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W, threshold[s[0]], update_steps, s[1])) #only the first pattern","metadata":{"execution":{"iopub.status.busy":"2022-07-28T13:13:07.160470Z","iopub.execute_input":"2022-07-28T13:13:07.160804Z","iopub.status.idle":"2022-07-28T14:47:24.317257Z","shell.execute_reply.started":"2022-07-28T13:13:07.160776Z","shell.execute_reply":"2022-07-28T14:47:24.316270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hit_ratio = torch.zeros((len(sparseness_ratio),len(nr_patterns)))\naverage_distance = torch.zeros((len(sparseness_ratio),len(nr_patterns)))\nquantile_distance = torch.zeros((len(sparseness_ratio),len(nr_patterns)))\nfor s in enumerate(sparseness_ratio):\n    for n in enumerate(nr_patterns):\n        hit_ratio[s[0],n[0]] = torch.sum(distances2[:,s[0],:,n[0]]<=tolerance[s[0]])/(iterations*batches)\n        average_distance[s[0],n[0]] = torch.mean(distances2[:,s[0],:,n[0]])\n        quantile_distance[s[0],n[0]] = np.quantile(distances2[:,s[0],:,n[0]], 0.9)\n#####        \n\nfig, axs = pl.subplots(3, 3, figsize=(15, 8), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.45, wspace=0.2)\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        axs[i,j].fill_between(nr_patterns/N, hit_ratio[i*axs.shape[1]+j,:], color='seagreen')\n        axs[i,j].plot(nr_patterns/N, hit_ratio[i*axs.shape[1]+j,:], color='darkgreen')\n        axs[i,j].set_title('p='+\"{:.0%}\".format(np.round(sparseness_ratio[i*axs.shape[1]+j],3)),size=14)\n        axs[i,j].set_ylim((-0.1, 1.1))\n        axs[i,j].grid(linewidth = 0.6)\n        if j==0: axs[i,j].set_ylabel('hit ratio', fontsize=14)\n        if i==2: axs[i,j].set_xlabel('load parameter', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T16:23:01.260946Z","iopub.execute_input":"2022-07-28T16:23:01.262008Z","iopub.status.idle":"2022-07-28T16:23:02.302731Z","shell.execute_reply.started":"2022-07-28T16:23:01.261957Z","shell.execute_reply":"2022-07-28T16:23:02.301840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. investigating and showing the discontinuous behaviour","metadata":{}},{"cell_type":"code","source":"# 5%\nb = np.append([0,0.005],np.linspace(0.01,0.5,99))\nfig, axs = pl.subplots(2, 3, figsize=(16, 6), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.37, wspace=0.17)\n\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        n, bins, patches = axs[i,j].hist(distances2[:,2,:,15+i*axs.shape[1]+j].reshape(1, iterations*batches), bins=b, color='seagreen')\n        axs[i,j].set_ylim([0,iterations*batches])\n        axs[i,j].set_title(r'$\\alpha$='+str(np.round(nr_patterns[15+i*axs.shape[1]+j]/N,2)),size=14)\n        axs[i,j].xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n        if j==0: axs[i,j].set_ylabel('frequency', fontsize=14)\n        if i==1: axs[i,j].set_xlabel('distance', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T16:48:22.090136Z","iopub.execute_input":"2022-07-28T16:48:22.090528Z","iopub.status.idle":"2022-07-28T16:48:23.499704Z","shell.execute_reply.started":"2022-07-28T16:48:22.090494Z","shell.execute_reply":"2022-07-28T16:48:23.498791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 10%\nb = np.append([0,0.005],np.linspace(0.01,0.5,99))\nfig, axs = pl.subplots(2, 3, figsize=(16, 6), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.37, wspace=0.17)\n\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        n, bins, patches = axs[i,j].hist(distances2[:,3,:,10+i*axs.shape[1]+j].reshape(1, iterations*batches), bins=b, color='seagreen')\n        axs[i,j].set_ylim([0,iterations*batches])\n        axs[i,j].set_title(r'$\\alpha$='+str(np.round(nr_patterns[10+i*axs.shape[1]+j]/N,2)),size=14)\n        axs[i,j].xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n        if j==0: axs[i,j].set_ylabel('frequency', fontsize=14)\n        if i==1: axs[i,j].set_xlabel('distance', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T16:49:19.720621Z","iopub.execute_input":"2022-07-28T16:49:19.720999Z","iopub.status.idle":"2022-07-28T16:49:21.303799Z","shell.execute_reply.started":"2022-07-28T16:49:19.720967Z","shell.execute_reply":"2022-07-28T16:49:21.302845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 50%\nb = np.append([0,0.005],np.linspace(0.01,0.5,99))\nfig, axs = pl.subplots(2, 3, figsize=(16, 6), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.37, wspace=0.17)\n\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        n, bins, patches = axs[i,j].hist(distances2[:,8,:,1+i*axs.shape[1]+j].reshape(1, iterations*batches), bins=b, color='seagreen')\n        axs[i,j].set_ylim([0,iterations*batches])\n        axs[i,j].set_title(r'$\\alpha$='+str(np.round(nr_patterns[1+i*axs.shape[1]+j]/N,2)),size=14)\n        axs[i,j].xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n        if j==0: axs[i,j].set_ylabel('frequency', fontsize=14)\n        if i==1: axs[i,j].set_xlabel('distance', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T16:49:54.301997Z","iopub.execute_input":"2022-07-28T16:49:54.302564Z","iopub.status.idle":"2022-07-28T16:49:55.873151Z","shell.execute_reply.started":"2022-07-28T16:49:54.302528Z","shell.execute_reply":"2022-07-28T16:49:55.872223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### the 90% percentile for the two examples","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(seed=7)","metadata":{"execution":{"iopub.status.busy":"2022-08-02T20:54:30.370520Z","iopub.execute_input":"2022-08-02T20:54:30.370910Z","iopub.status.idle":"2022-08-02T20:54:30.376916Z","shell.execute_reply.started":"2022-08-02T20:54:30.370878Z","shell.execute_reply":"2022-08-02T20:54:30.375638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nr_patterns = (np.linspace(0.52, 0.58, 12, dtype=np.float16)*N).astype(int)\nsparseness_ratio = 0.05\nthreshold = (sparseness_ratio**3-sparseness_ratio**2 + sparseness_ratio-2*sparseness_ratio**2+sparseness_ratio**3)/2\n\ndistances2_1 = torch.zeros((batches,iterations,len(nr_patterns)) ,dtype=torch.float16)\nfor b in range(batches):    \n    print(b)\n    for n in enumerate(nr_patterns):\n        patterns, W = set_up_the_model(iterations, N, int(n[1]), sparseness_ratio)\n        distances2_1[b,:,n[0]] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W, threshold, update_steps, sparseness_ratio)) #only the first pattern","metadata":{"execution":{"iopub.status.busy":"2022-08-02T20:54:32.076810Z","iopub.execute_input":"2022-08-02T20:54:32.077517Z","iopub.status.idle":"2022-08-02T21:00:03.093915Z","shell.execute_reply.started":"2022-08-02T20:54:32.077481Z","shell.execute_reply":"2022-08-02T21:00:03.092925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nr_patterns = (np.linspace(0.1, 0.16, 12, dtype=np.float16)*N).astype(int)\nsparseness_ratio = 0.5\nthreshold = (sparseness_ratio**3-sparseness_ratio**2 + sparseness_ratio-2*sparseness_ratio**2+sparseness_ratio**3)/2\n\ndistances2_2 = torch.zeros((batches,iterations,len(nr_patterns)) ,dtype=torch.float16)\nfor b in range(batches):    \n    print(b)\n    for n in enumerate(nr_patterns):\n        patterns, W = set_up_the_model(iterations, N, int(n[1]), sparseness_ratio)\n        distances2_2[b,:,n[0]] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W, threshold, update_steps, sparseness_ratio)) #only the first pattern","metadata":{"execution":{"iopub.status.busy":"2022-08-02T21:00:03.095633Z","iopub.execute_input":"2022-08-02T21:00:03.095962Z","iopub.status.idle":"2022-08-02T21:03:00.678890Z","shell.execute_reply.started":"2022-08-02T21:00:03.095928Z","shell.execute_reply":"2022-08-02T21:03:00.677926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.rcParams.update({'font.size': 11})\npercentile_distance1 = np.zeros(12)\npercentile_distance2 = np.zeros(12)\nfor n in range(12):\n    percentile_distance1[n] = np.quantile(distances2_1[:,:,n],0.9)\n    percentile_distance2[n] = np.quantile(distances2_2[:,:,n],0.9)\n#####\nfig, axs = pl.subplots(1, 2, figsize=(12, 5), facecolor='w', edgecolor='k')\nfig.subplots_adjust(wspace=0.3)\n\nnr_patterns = (np.linspace(0.52, 0.58, 12, dtype=np.float16)*N).astype(int)\naxs[0].plot(nr_patterns/N, percentile_distance1, color='darkgreen', linewidth=2)\naxs[0].set_title('p='+\"{:.0%}\".format(0.05),size=14)\n#axs[0].set_xlim((0.1, 0.16))\naxs[0].set_ylim((0, 0.4))\naxs[0].set_xlabel('load parameter', fontsize=14)\naxs[0].set_ylabel('90th percentile of distance', fontsize=14)\nfirst_derivative = np.diff(percentile_distance1)\nsecond_derivative = np.diff(np.diff(percentile_distance1))\nmax_curvature = nr_patterns[np.where(second_derivative==np.max(second_derivative))[0]+1]/N\nmax_curvature_dist = percentile_distance1[np.where(second_derivative==np.max(second_derivative))[0]+1]\nprint(max_curvature)\nprint(max_curvature_dist)\nprint(nr_patterns[np.where(first_derivative==np.max(first_derivative))[0]]/N)\naxs[0].axvline(x=max_curvature, color='red', linestyle='--', linewidth=1)\naxs[0].text(max_curvature-0.003,0.1,'$\\\\alpha$='+' '.join(map(str, np.round(max_curvature,4))),rotation=90, size=12)\naxs[0].axhline(y=max_curvature_dist, color='red', linestyle='--', linewidth=1)\naxs[0].text(0.525,max_curvature_dist+0.006,'distance='+' '.join(map(str, np.round(max_curvature_dist,4))), size=12)\n\n\nnr_patterns = (np.linspace(0.1, 0.16, 12, dtype=np.float16)*N).astype(int)\naxs[1].plot(nr_patterns/N, percentile_distance2, color='darkgreen', linewidth=2)\naxs[1].set_title('p='+\"{:.0%}\".format(0.5),size=14)\naxs[1].set_ylim((0, 0.4))\naxs[1].set_xlabel('load parameter', fontsize=14)\naxs[1].set_ylabel('90th percentile of distance', fontsize=14)\nfirst_derivative = np.diff(percentile_distance2)\nsecond_derivative = np.diff(np.diff(percentile_distance2))\nmax_curvature = nr_patterns[np.where(second_derivative==np.max(second_derivative))[0]+1]/N\nmax_curvature_dist = percentile_distance2[np.where(second_derivative==np.max(second_derivative))[0]+1]\nprint(max_curvature)\nprint(max_curvature_dist)\nprint(nr_patterns[np.where(first_derivative==np.max(first_derivative))[0]]/N)\naxs[1].axvline(x=max_curvature, color='red', linestyle='--', linewidth=1)\naxs[1].text(max_curvature-0.003,0.1,'$\\\\alpha$='+' '.join(map(str, np.round(max_curvature,4))),rotation=90, size=12)\naxs[1].axhline(y=max_curvature_dist, color='red', linestyle='--', linewidth=1)\naxs[1].text(0.105,max_curvature_dist+0.006,'distance='+' '.join(map(str, np.round(max_curvature_dist,4))), size=12)\npl.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-02T21:22:59.487790Z","iopub.execute_input":"2022-08-02T21:22:59.488178Z","iopub.status.idle":"2022-08-02T21:22:59.860849Z","shell.execute_reply.started":"2022-08-02T21:22:59.488144Z","shell.execute_reply":"2022-08-02T21:22:59.859937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. (example) dependence of performance on weight dilution for different sparseness values","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(seed=7)\n\nnr_patterns = int(0.12*N)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T13:05:15.972089Z","iopub.execute_input":"2022-07-29T13:05:15.972954Z","iopub.status.idle":"2022-07-29T13:05:15.978040Z","shell.execute_reply.started":"2022-07-29T13:05:15.972920Z","shell.execute_reply":"2022-07-29T13:05:15.976827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"noise = torch.linspace(0, 1, 21)\n\ndistances3 = torch.zeros((batches, len(sparseness_ratio), iterations, len(noise)))\nfor b in range(batches):\n    print(b)\n    for s in enumerate(sparseness_ratio):\n        patterns, W = set_up_the_model(iterations, N, nr_patterns, s[1])\n        random_matrix = torch.rand(iterations,N,N).cuda()\n        for n in enumerate(noise):\n            W_n = dilute_connections(W, n[1], random_matrix)\n            distances3[b,s[0],:,n[0]] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W_n, threshold[s[0]], update_steps, s[1]))","metadata":{"execution":{"iopub.status.busy":"2022-07-29T13:05:18.981643Z","iopub.execute_input":"2022-07-29T13:05:18.982716Z","iopub.status.idle":"2022-07-29T13:14:30.167748Z","shell.execute_reply.started":"2022-07-29T13:05:18.982669Z","shell.execute_reply":"2022-07-29T13:14:30.166826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.rcParams.update({'font.size': 11})\nhit_ratio = torch.zeros((len(sparseness_ratio),len(noise)))\naverage_distance = torch.zeros((len(sparseness_ratio),len(noise)))\nfor s in enumerate(sparseness_ratio):\n    for n in enumerate(noise):\n        hit_ratio[s[0],n[0]] = torch.sum(distances3[:,s[0],:,n[0]]<=tolerance[s[0]])/(iterations*batches)\n        average_distance[s[0],n[0]] = torch.mean(distances3[:,s[0],:,n[0]])\n#####        \nfig, axs = pl.subplots(3, 3, figsize=(15, 8), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.45, wspace=0.2)\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        axs[i,j].fill_between(noise, hit_ratio[i*axs.shape[1]+j,:], color='seagreen')\n        axs[i,j].plot(noise, hit_ratio[i*axs.shape[1]+j,:], color='darkgreen')\n        axs[i,j].set_title('p='+\"{:.0%}\".format(np.round(sparseness_ratio[i*axs.shape[1]+j],3)),size=14)\n        axs[i,j].set_ylim((-0.1, 1.1))\n        axs[i,j].grid(linewidth = 0.6)\n        if j==0: axs[i,j].set_ylabel('hit ratio', fontsize=14)\n        if i==2: axs[i,j].set_xlabel('dilution probability', fontsize=14)\n#####\nfig, axs = pl.subplots(1, 1, figsize=(6, 4), facecolor='w', edgecolor='k')\naxs.plot(sparseness_ratio, calc_auc(hit_ratio, noise, 1), '-s', color='darkgreen')\naxs.set_xlabel(\"mean activity level (p)\",size=14)\naxs.set_ylabel(\"normalized AUC\",size=14)\naxs.set_title(\"Area Under Curve\",size=15)\naxs.set_ylim([0,0.6])\naxs.grid(linewidth = 0.6)\naxs.xaxis.set_major_formatter(mtick.PercentFormatter(1.0)) \npl.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-29T16:45:13.383205Z","iopub.execute_input":"2022-07-29T16:45:13.383581Z","iopub.status.idle":"2022-07-29T16:45:14.393639Z","shell.execute_reply.started":"2022-07-29T16:45:13.383549Z","shell.execute_reply":"2022-07-29T16:45:14.392719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = pl.subplots(3, 3, figsize=(15, 8), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.45, wspace=0.2)\nfor i in range(axs.shape[0]):\n    for j in range(axs.shape[1]):\n        axs[i,j].fill_between(noise, average_distance[i*axs.shape[1]+j,:], color=\"seagreen\")\n        axs[i,j].plot(noise, average_distance[i*axs.shape[1]+j,:], color=\"darkgreen\")\n        axs[i,j].set_title('p='+\"{:.0%}\".format(np.round(sparseness_ratio[i*axs.shape[1]+j],3)),size=14)\n        axs[i,j].set_ylim((-0.1, 1.1))\n        axs[i,j].grid(linewidth = 0.6)\n        if j==0: axs[i,j].set_ylabel('average distance', fontsize=14)\n        if i==2: axs[i,j].set_xlabel('dilution probability', fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T13:27:47.300273Z","iopub.execute_input":"2022-07-29T13:27:47.301276Z","iopub.status.idle":"2022-07-29T13:27:48.376641Z","shell.execute_reply.started":"2022-07-29T13:27:47.301226Z","shell.execute_reply":"2022-07-29T13:27:48.375654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. robustness","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(seed=7)\n\nnr_patterns = (np.array([0.05,0.1,0.15,0.2,0.3,0.4,0.5])*N).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T17:08:09.690292Z","iopub.execute_input":"2022-08-07T17:08:09.690807Z","iopub.status.idle":"2022-08-07T17:08:09.700223Z","shell.execute_reply.started":"2022-08-07T17:08:09.690774Z","shell.execute_reply":"2022-08-07T17:08:09.698771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nr_flips = torch.linspace(0, N, 21)\nnoise_w = torch.linspace(0, 0.002, 20)\nnoise_w_dil = torch.linspace(0, 1, 21)\nnoise_n_dil = torch.linspace(0, 0.9, 19)\nnoise_stoc = torch.linspace(0, 1, 21)\n\ndistances_f = torch.zeros((batches,len(nr_patterns), len(sparseness_ratio), len(nr_flips), iterations))\ndistances_f1 = torch.zeros((batches,len(nr_patterns), len(sparseness_ratio), len(nr_flips), iterations))\ndistances_f2 = torch.zeros((batches,len(nr_patterns), len(sparseness_ratio), len(nr_flips), iterations))\ndistances_n1 = torch.zeros((batches,len(nr_patterns), len(sparseness_ratio), len(noise_w), iterations))\ndistances_n2 = torch.zeros((batches,len(nr_patterns), len(sparseness_ratio), len(noise_w_dil), iterations))\ndistances_n3 = torch.zeros((batches,len(nr_patterns), len(sparseness_ratio), len(noise_n_dil), iterations))\ndistances_stoch = torch.zeros((batches,len(nr_patterns), len(sparseness_ratio), len(noise_stoc), iterations))\n\nfor b in range(batches):\n    for pat in enumerate(nr_patterns):\n        print(pat[0])\n        for s in enumerate(sparseness_ratio):\n            patterns, W = set_up_the_model(iterations, N, pat[1], s[1])\n            random_matrix_norm = torch.normal(0,1,(iterations,N,N)).cuda()\n            random_matrix_unif = torch.rand(iterations,N,N).cuda()\n            for f in enumerate(nr_flips):\n                distances_f[b,pat[0],s[0],f[0],:] = patterns_distance(patterns[:,:,0], update_network(disturb_a_pattern(patterns[:,:,0], int(f[1])).unsqueeze(-1), W, threshold[s[0]], update_steps, s[1]))\n                distances_f1[b,pat[0],s[0],f[0],:] = patterns_distance(patterns[:,:,0], update_network(disturb_a_pattern(patterns[:,:,0], int(f[1]), 'on').unsqueeze(-1), W, threshold[s[0]], update_steps, s[1]))\n                distances_f2[b,pat[0],s[0],f[0],:] = patterns_distance(patterns[:,:,0], update_network(disturb_a_pattern(patterns[:,:,0], int(f[1]), 'off').unsqueeze(-1), W, threshold[s[0]], update_steps, s[1]))\n            for n in enumerate(noise_w):\n                W_n = W+random_matrix_norm*n[1]\n                W_n = W_n*(1-torch.eye(N,N).cuda())\n                distances_n1[b,pat[0],s[0],n[0],:] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W_n, threshold[s[0]], update_steps, s[1]))\n            for n in enumerate(noise_w_dil):\n                W_n = dilute_connections(W, n[1], random_matrix_unif)\n                distances_n2[b,pat[0],s[0],n[0],:] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W_n, threshold[s[0]], update_steps, s[1]))\n            for n in enumerate(noise_n_dil):\n                patterns_n    = dilute_nodes_p(patterns, n[1])\n                W_n           = dilute_nodes_W(W, n[1])\n                distances_n3[b,pat[0],s[0],n[0],:] = patterns_distance(patterns_n[:,:,0], update_network(patterns_n[:,:,0].unsqueeze(-1), W_n, threshold[s[0]], update_steps, s[1]))\n                index_pattern_n_0 = torch.where(torch.sum(patterns_n[:,:,0]==1,1)==0)[0].cuda()\n                distances_n3[b,pat[0],s[0],n[0],index_pattern_n_0] = 1\n            for n in enumerate(noise_stoc):\n                distances_stoch[b,pat[0],s[0],n[0],:] = patterns_distance(patterns[:,:,0], update_network(patterns[:,:,0].unsqueeze(-1), W, threshold[s[0]], update_steps, s[1], 'on', n[1]))","metadata":{"execution":{"iopub.status.busy":"2022-08-07T17:08:11.718522Z","iopub.execute_input":"2022-08-07T17:08:11.719006Z","iopub.status.idle":"2022-08-07T22:02:53.463208Z","shell.execute_reply.started":"2022-08-07T17:08:11.718950Z","shell.execute_reply":"2022-08-07T22:02:53.461855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hit_ratio_flip = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(nr_flips)))\nhit_ratio_on = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(nr_flips)))\nhit_ratio_off = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(nr_flips)))\nhit_ratio_w = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_w)))\nhit_ratio_w_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_w_dil)))\nhit_ratio_n_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_n_dil)))\nhit_ratio_stoc = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_stoc)))\n\naverage_distance_flip = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(nr_flips)))\naverage_distance_on = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(nr_flips)))\naverage_distance_off = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(nr_flips)))\naverage_distance_w = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_w)))\naverage_distance_w_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_w_dil)))\naverage_distance_n_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_n_dil)))\naverage_distance_stoc = torch.zeros((len(nr_patterns),len(sparseness_ratio),len(noise_stoc)))\n\nAUC_flip = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nAUC_on = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nAUC_off = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nAUC_w = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nAUC_w_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nAUC_n_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nAUC_stoc = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\n\nDP_flip = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nDP_on = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nDP_off = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nDP_w = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nDP_w_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nDP_n_dil = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nDP_stoc = torch.zeros((len(nr_patterns),len(sparseness_ratio)))\nDR_threshold = 0.95\n\nfor pat in enumerate(nr_patterns):\n    for s in enumerate(sparseness_ratio):\n        for f in enumerate(nr_flips):\n            hit_ratio_flip[pat[0],s[0],f[0]] = torch.sum(distances_f[:,pat[0],s[0],f[0],:]<=tolerance[s[0]])/(iterations*batches)\n            hit_ratio_on[pat[0],s[0],f[0]] = torch.sum(distances_f1[:,pat[0],s[0],f[0],:]<=tolerance[s[0]])/(iterations*batches)\n            hit_ratio_off[pat[0],s[0],f[0]] = torch.sum(distances_f2[:,pat[0],s[0],f[0],:]<=tolerance[s[0]])/(iterations*batches)\n            average_distance_flip[pat[0],s[0],f[0]] = torch.mean(distances_f[:,pat[0],s[0],f[0],:])\n            average_distance_on[pat[0],s[0],f[0]] = torch.mean(distances_f1[:,pat[0],s[0],f[0],:])\n            average_distance_off[pat[0],s[0],f[0]] = torch.mean(distances_f2[:,pat[0],s[0],f[0],:])            \n\n        for n in enumerate(noise_w):\n            hit_ratio_w[pat[0],s[0],n[0]] = torch.sum(distances_n1[:,pat[0],s[0],n[0],:]<=tolerance[s[0]])/(iterations*batches)\n            average_distance_w[pat[0],s[0],n[0]] = torch.mean(distances_n1[:,pat[0],s[0],n[0],:])\n        for n in enumerate(noise_w_dil):\n            hit_ratio_w_dil[pat[0],s[0],n[0]] = torch.sum(distances_n2[:,pat[0],s[0],n[0],:]<=tolerance[s[0]])/(iterations*batches)\n            average_distance_w_dil[pat[0],s[0],n[0]] = torch.mean(distances_n2[:,pat[0],s[0],n[0],:])\n        for n in enumerate(noise_n_dil):\n            hit_ratio_n_dil[pat[0],s[0],n[0]] = torch.sum(distances_n3[:,pat[0],s[0],n[0],:]<=tolerance[s[0]])/(iterations*batches)\n            average_distance_n_dil[pat[0],s[0],n[0]] = torch.mean(distances_n3[:,pat[0],s[0],n[0],:])\n        for n in enumerate(noise_stoc):\n            hit_ratio_stoc[pat[0],s[0],n[0]] = torch.sum(distances_stoch[:,pat[0],s[0],n[0],:]<=tolerance[s[0]])/(iterations*batches)\n            average_distance_stoc[pat[0],s[0],n[0]] = torch.mean(distances_stoch[:,pat[0],s[0],n[0],:])        \n    AUC_flip[pat[0],:] = np.round(calc_auc(hit_ratio_flip[pat[0],:,:], nr_flips, 1), 2)\n    AUC_on[pat[0],:] = np.round(calc_auc(hit_ratio_on[pat[0],:,:], nr_flips, 1), 2)\n    AUC_off[pat[0],:] = np.round(calc_auc(hit_ratio_off[pat[0],:,:], nr_flips, 1), 2)\n    AUC_w[pat[0],:] = np.round(calc_auc(hit_ratio_w[pat[0],:,:], noise_w, 1), 2)\n    AUC_w_dil[pat[0],:] = np.round(calc_auc(hit_ratio_w_dil[pat[0],:,:], noise_w_dil, 1), 2)\n    AUC_n_dil[pat[0],:] = np.round(calc_auc(hit_ratio_n_dil[pat[0],:,:], noise_n_dil, 1), 2)\n    AUC_stoc[pat[0],:] = np.round(calc_auc(hit_ratio_stoc[pat[0],:,:], noise_stoc, 1), 2)\n    \n    DP_flip[pat[0],:] = calc_dropping_point(hit_ratio_flip[pat[0],:,:], nr_flips, DR_threshold)\n    DP_on[pat[0],:] = calc_dropping_point(hit_ratio_on[pat[0],:,:], nr_flips, DR_threshold)\n    DP_off[pat[0],:] = calc_dropping_point(hit_ratio_off[pat[0],:,:], nr_flips, DR_threshold)\n    DP_w[pat[0],:] = calc_dropping_point(hit_ratio_w[pat[0],:,:], noise_w, DR_threshold)\n    DP_w_dil[pat[0],:] = calc_dropping_point(hit_ratio_w_dil[pat[0],:,:], noise_w_dil, DR_threshold)\n    DP_n_dil[pat[0],:] = calc_dropping_point(hit_ratio_n_dil[pat[0],:,:], noise_n_dil, DR_threshold)\n    DP_stoc[pat[0],:] = calc_dropping_point(hit_ratio_stoc[pat[0],:,:], noise_stoc, DR_threshold)","metadata":{"execution":{"iopub.status.busy":"2022-08-07T22:06:00.967080Z","iopub.execute_input":"2022-08-07T22:06:00.967633Z","iopub.status.idle":"2022-08-07T22:06:02.267910Z","shell.execute_reply.started":"2022-08-07T22:06:00.967587Z","shell.execute_reply":"2022-08-07T22:06:02.266442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.rcParams.update({'font.size': 12})\ncolors = np.array(['tab:blue', 'tab:red', 'tab:green', 'tab:purple', 'tab:olive', 'tab:cyan', 'tab:brown'])\nfig, axs = pl.subplots(4, 2, figsize=(15, 20), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.4, wspace=0.14)\nfig.delaxes(axs[3,1])\nfor pat in enumerate(nr_patterns):\n    axs[0,0].plot(sparseness_ratio, AUC_stoc[pat[0],:], '-s', label=str(pat[1]/N), color=colors[pat[0]])\n    axs[0,1].plot(sparseness_ratio, AUC_w[pat[0],:], '-s', label=str(pat[1]/N), color=colors[pat[0]])\n    axs[1,0].plot(sparseness_ratio, AUC_w_dil[pat[0],:], '-s', label=str(pat[1]/N), color=colors[pat[0]])\n    axs[1,1].plot(sparseness_ratio, AUC_n_dil[pat[0],:], '-s', label=str(pat[1]/N), color=colors[pat[0]])\n    axs[2,0].plot(sparseness_ratio, AUC_flip[pat[0],:], '-s', label=str(pat[1]/N), color=colors[pat[0]])\n    axs[2,1].plot(sparseness_ratio, AUC_on[pat[0],:], '-s', label=str(pat[1]/N), color=colors[pat[0]])    \n    axs[3,0].plot(sparseness_ratio, AUC_off[pat[0],:], '-s', label=str(pat[1]/N), color=colors[pat[0]])    \n    \nfor i in range(4):    \n    for j in range(2):    \n        #axs[i,j].set_ylim([0,1])\n        axs[i,j].xaxis.set_major_formatter(mtick.PercentFormatter(1.0)) \n        axs[i,j].set_xlabel(\"mean activity level (p)\", size=14)\n        if j==0:\n            axs[i,j].set_ylabel(\"normalized AUC\", size=14)\n        if j==1 or i==3:\n            axs[i,j].legend(bbox_to_anchor =(1, 1))\n\naxs[0,0].set_title(\"robustness against stochastic threshold\", size=16)\naxs[0,1].set_title(\"robustness against weight noise\", size=16)\naxs[1,0].set_title(\"robustness against connection loss\", size=16)\naxs[1,1].set_title(\"robustness against node loss\", size=16)\naxs[2,0].set_title(\"robustness against state flips\", size=16)\naxs[2,1].set_title(\"robustness against states turning on\", size=16)\naxs[3,0].set_title(\"robustness against states turning off\", size=16)\n\npl.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-07T22:03:30.100123Z","iopub.execute_input":"2022-08-07T22:03:30.100608Z","iopub.status.idle":"2022-08-07T22:03:31.399556Z","shell.execute_reply.started":"2022-08-07T22:03:30.100577Z","shell.execute_reply":"2022-08-07T22:03:31.398261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfig, axs = pl.subplots(4, 2, figsize=(16, 20), facecolor='w', edgecolor='k')\nfig.subplots_adjust(hspace=0.4, wspace=0.12)\nfig.delaxes(axs[3,1])\nsns.heatmap(AUC_stoc, linewidth=1, annot=True, xticklabels=list(map(\"{:.0%}\".format, sparseness_ratio)), yticklabels=np.round(nr_patterns/N,2), cbar=False, ax=axs[0,0])\nsns.heatmap(AUC_w,    linewidth=1, annot=True, xticklabels=list(map(\"{:.0%}\".format, sparseness_ratio)), yticklabels=np.round(nr_patterns/N,2), cbar=False, ax=axs[0,1])\nsns.heatmap(AUC_w_dil,linewidth=1, annot=True, xticklabels=list(map(\"{:.0%}\".format, sparseness_ratio)), yticklabels=np.round(nr_patterns/N,2), cbar=False, ax=axs[1,0])\nsns.heatmap(AUC_n_dil,linewidth=1, annot=True, xticklabels=list(map(\"{:.0%}\".format, sparseness_ratio)), yticklabels=np.round(nr_patterns/N,2), cbar=False, ax=axs[1,1])\nsns.heatmap(AUC_flip, linewidth=1, annot=True, xticklabels=list(map(\"{:.0%}\".format, sparseness_ratio)), yticklabels=np.round(nr_patterns/N,2), cbar=False, ax=axs[2,0])\nsns.heatmap(AUC_on,   linewidth=1, annot=True, xticklabels=list(map(\"{:.0%}\".format, sparseness_ratio)), yticklabels=np.round(nr_patterns/N,2), cbar=False, ax=axs[2,1])\nsns.heatmap(AUC_off,  linewidth=1, annot=True, xticklabels=list(map(\"{:.0%}\".format, sparseness_ratio)), yticklabels=np.round(nr_patterns/N,2), cbar=False, ax=axs[3,0])\n\nfor i in range(4):    \n    for j in range(2):    \n        #axs[i,j].legend()\n        axs[i,j].invert_yaxis()\n        axs[i,j].set_xlabel(\"mean activity level (p)\", size=14)\n        if j==0:\n            axs[i,j].set_ylabel(\"load parameter\", size=14)\n\naxs[0,0].set_title(\"robustness against stochastic threshold\", size=16)\naxs[0,1].set_title(\"robustness against weight noise\", size=16)\naxs[1,0].set_title(\"robustness against connection loss\", size=16)\naxs[1,1].set_title(\"robustness against node loss\", size=16)\naxs[2,0].set_title(\"robustness against state flips\", size=16)\naxs[2,1].set_title(\"robustness against states turning on\", size=16)\naxs[3,0].set_title(\"robustness against states turning off\", size=16)\n\npl.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-07T22:06:33.025134Z","iopub.execute_input":"2022-08-07T22:06:33.025561Z","iopub.status.idle":"2022-08-07T22:06:38.664305Z","shell.execute_reply.started":"2022-08-07T22:06:33.025528Z","shell.execute_reply":"2022-08-07T22:06:38.662750Z"},"trusted":true},"execution_count":null,"outputs":[]}]}